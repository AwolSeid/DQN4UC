# MARLSE4MOPS: Multi-Agent Reinforcement Learning Simulation Environment for Multi-Objective Power Scheduling

### Introduction
To meet the time-varying and over time increasing demand for electrical energy, different power generating units with various operational and financial constraints are being used. In doing so, the first task of power scheduling deals with economic cost dispatch (ECD): finding an optimal power dispatching schedule with the minimum possible cost while satisfying the different power system constraints. On the other hand, two-thirds of electric energy is currently generated from fossil fuels (Rex & Marsaline, 2017). Fossil fuels are the major sources of different greenhouse gases (GHGs) and environmental pollutants such as carbon dioxide (CO2) and sulfur dioxide (SO2). This means electricity generation is the major contributor to climate change, global warming, and air pollution. In addition to the minimum economic cost of operation, environmental emission dispatch (EED) is another important task of power operation. However, the single objective cost and emission-based dispatches are conflicting because the minimization of one causes the other to increase. Therefore, the Multi-Objective Power Scheduling (MOPS) problem focuses on simultaneously minimizing both the economic cost and environmental emissions, and finding a solution that balances the two.
	Many model-based optimization approaches and methods have been proposed to solve MOPS problems. However, due to the combinatorial nature of commitments, nonconvex cost and emission characteristics, and multi-period unit-specific and system-level constraints (Zaoui & Belmadani, 2022), most of the studies dealt with systems with 10 or fewer units (de Mars & O'Sullivan, 2021). Besides, the costs and emissions are estimated using the usual quadratic curves in most studies. However, the input-output characteristics of thermal units are inherently non-smooth and discontinuous because of the valve-point loading effects and ramp-rate constraints (Roy, et al., 2013; Wang, et al., 2017). A few studies, such as (Balasubramanian & Santhi, 2016) and (Rajasomashekar & Aravindhababu, 2012), solved systems up to 100 units. Though these studies considered a larger number of units, the most important ramp rate constraints and valve point loading effects of thermal units were compromised. Hence, the smooth quadratic approximations of the cost and emission curves are not valid (Roy, et al., 2013). The consideration of ramp rate constraints and valve point loading effects is essential for more accurate modeling (Roy, et al., 2013).
	The recent advancement of artificial intelligence (AI) has demonstrated the capability of RL to make intelligent decisions by learning and adapting through trial-and-error interactions with a dynamic environment (de Mars & O'Sullivan, 2021). There are exceptionally few RL-based methods recognized for solving single-objective power scheduling (SOPS) problems (Mars & O'Sullivan, 2022). However, to the best of our knowledge, there has been no previous study at all that has explored the RL methodology to address the complex MOPS problem. By harnessing the capabilities of AI, this study breaks new ground by offering a novel and pioneering multi-agent reinforcement learning (MARL) approach to solve the challenges of MOPS problems.
### Simulation Framework
Within the framework of MARL, the generating units are represented as multi-RL agents, each with different unit-specific characteristics and multiple conflicting objectives. Each agent can determine its corresponding commitment and load dispatch independently of others. However, the joint decision of all agents should satisfy the demand at each period of the scheduling horizon and then minimize the total operation cost and emission level of the entire planning horizon. As a result, the agents are cooperative types of multi-RL agents (Zhang, et al., 2021). 
	The framework MARL manifests the form of state $\cal S$, action  $\cal A$, transition (probability) function  $\cal P$ and reward  $\cal R$ for a sequence of discrete timesteps $t$. Accordingly, the following section formalizes the MOPS problem in terms of these key components.
#### Components of the MARL Framework

Description of the MOPS dynamics in the framework of MARL. 
For Since the scheduling horizon is an hourly divided day, each hour of a day is considered a timestep t,∀t.  Hence, one cycle of determination of commitments and load dispatches for a day represents a complete episode. 
- \textbf{State Space} ($\cal S$): For each timestep $t$ of an episode, the system will be in a state $\cal{s}_t$ that consists of the timestep $t$, minimum capacities ($p_{it}^{min};\forall i$) and maximum capacities ($p_{it}^{max};\forall i$)  based on the maximum ramp down rates ($p_{(i*)}^{down};\forall i$) and ramp up rates ($p_{(i*)}^{up};\forall i$), current operating (online/offline) durations ($t_{it};\forall i$) of the units based on the minimum online duration ($t_{(i*)}^{up};\forall i$) and offline duration ($t_{(i*)}^{down};\forall i$) of the units, and the demand ($d_t$) to be satisfied. The state $s_t$ at timestep $t$ is defined as $s_t=(t,p_t^min,p_t^max,t_t,d_t)$ where $t$ is the current timestep, $p_t^min$ is a vector of minimum capacities, $p_t^max$ is a vector of maximum capacities, $t_t$ is a vector of current (online/offline) duration; and $d_t$ is the demand. Overall, the system’s state space can be described as: $S=(t,P^min,P^max,T,d)$. 
- Action Space (A): Each of the n agents have two possible actions (switch-to/stay ON or switch-to/stay OFF), that is, a_it∈{0,1} at timestep t (or in state s_t). This implies, there are a total of 2^n commitments in the action space A. The decisions of all the n agents constitute an n-dimensional vector a_t={0,1}^n∈A. This actions vector a_t is the change of the switch (ON/OFF) status of units between timesteps t and t+1.
- Transition Function {P(s_t^' |s_t,a_t )}: Once the agents take actions a_t∈A in the current state s_t∈S, there is a transition (or probability) function P(s_t^' |s_t,a_t ) leading to the next state s_t^'. The transition function must satisfy all the constraints from Eqs. (9)-(13) at each timestep t,∀t∈T. If any of the constraints are violated, it would not be legitimate to advance to the succeeding state s_t^'=s_(t+1). The legality and viability of each agent action and the joint actions must be verified before moving on to the next state s_(t+1). To do this, (Qin, et al., 2021) examined a brute force way of checking each of the possible commitments. Such an approach is computationally expensive and not practical for a system with a larger number of units and/or constraints. On the other hand, (Navin & Sharma, 2019) had considered the penalty for the last unit only as a slack item. However, such a single unit adjustment may not be enough to increase or decrease the supply capacity to the needed level. Consequently, this method of adjustment may also lengthen the learning process. In this study, instead, an auto-corrective simulation environment is built using the idea of contextual search (Sutton & Barto, 2018) to make context-based adjustments and corrections on any illegal agents’ decisions or infeasible combined actions that are against the state. For instance, the small capacity of the agents will be adjusted by switching each of the unconstrained OFF agents back to ON in an increasing order of the priority list described in Step 4.2 of Section 3.2. Similarly, the same applies to adjusting for excess capacity.
- Reward function (R): In order to evaluate the effectiveness of the agents, it is necessary to observe the reinforcement signal as a performance measure of the agents’ decisions. Based on the state s_t∈S, the agents get a reward based on a predefined reinforcement function  R(s_t,a_t,s_t^' ) described in Step 4.7 of the next section. The reward in this study is defined as the inverse of the normalized operation (production, startup, and shutdown) values of each objective function. 
	Hence, the MOPS dynamics can be formally formulated as a 4-tuple (S,A,P,R) Markov Decision Process (MDP) since the framework of MARL satisfies the properties of MDP (Sutton & Barto, 2018). Therefore, the dynamics of the MOPS can be simulated in the form of MDPs in the custom agent-based simulation environment, whose main components are described below.
#####



 

$\frac{2}{3}$

Multi-Agent Reinforcement Learning Simulation Environment for Power Scheduling

This environment is used to simulate mono- to tri-objective power scheduling problems dynamics in the form of Markov Decision Processes (MDPs).
To tackle the dimensionality challenges associated with MOPS, 
- Illegal actions by agents are to be automatically corrected.
- Shortages or excess capacities are adjusted.
The MDPS can then be used to train a custom deep learning model.
The practical viability of the environment is evaluated on different test systems featuring mono- to tri-objective problems.
