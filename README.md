# MARLSE4MOPS: Multi-Agent Reinforcement Learning Simulation Environment for Multi-Objective Power Scheduling

### Introduction
To meet the time-varying and over time increasing demand for electrical energy, different power generating units with various operational and financial constraints are being used. In doing so, the first task of power scheduling deals with economic cost dispatch (ECD): finding an optimal power dispatching schedule with the minimum possible cost while satisfying the different power system constraints. On the other hand, two-thirds of electric energy is currently generated from fossil fuels (Rex & Marsaline, 2017). Fossil fuels are the major sources of different greenhouse gases (GHGs) and environmental pollutants such as carbon dioxide (CO2) and sulfur dioxide (SO2). This means electricity generation is the major contributor to climate change, global warming, and air pollution. In addition to the minimum economic cost of operation, environmental emission dispatch (EED) is another important task of power operation. However, the single objective cost and emission-based dispatches are conflicting because the minimization of one causes the other to increase. Therefore, the Multi-Objective Power Scheduling (MOPS) problem focuses on simultaneously minimizing both the economic cost and environmental emissions, and finding a solution that balances the two.
	Many model-based optimization approaches and methods have been proposed to solve MOPS problems. However, due to the combinatorial nature of commitments, nonconvex cost and emission characteristics, and multi-period unit-specific and system-level constraints (Zaoui & Belmadani, 2022), most of the studies dealt with systems with 10 or fewer units (de Mars & O'Sullivan, 2021). Besides, the costs and emissions are estimated using the usual quadratic curves in most studies. However, the input-output characteristics of thermal units are inherently non-smooth and discontinuous because of the valve-point loading effects and ramp-rate constraints (Roy, et al., 2013; Wang, et al., 2017). A few studies, such as (Balasubramanian & Santhi, 2016) and (Rajasomashekar & Aravindhababu, 2012), solved systems up to 100 units. Though these studies considered a larger number of units, the most important ramp rate constraints and valve point loading effects of thermal units were compromised. Hence, the smooth quadratic approximations of the cost and emission curves are not valid (Roy, et al., 2013). The consideration of ramp rate constraints and valve point loading effects is essential for more accurate modeling (Roy, et al., 2013).
	The recent advancement of artificial intelligence (AI) has demonstrated the capability of RL to make intelligent decisions by learning and adapting through trial-and-error interactions with a dynamic environment (de Mars & O'Sullivan, 2021). There are exceptionally few RL-based methods recognized for solving single-objective power scheduling (SOPS) problems (Mars & O'Sullivan, 2022). However, to the best of our knowledge, there has been no previous study at all that has explored the RL methodology to address the complex MOPS problem. By harnessing the capabilities of AI, this study breaks new ground by offering a novel and pioneering multi-agent reinforcement learning (MARL) approach to solve the challenges of MOPS problems.
### 
$\frac{2}{3}$

Multi-Agent Reinforcement Learning Simulation Environment for Power Scheduling

This environment is used to simulate mono- to tri-objective power scheduling problems dynamics in the form of Markov Decision Processes (MDPs).
To tackle the dimensionality challenges associated with MOPS, 
- Illegal actions by agents are to be automatically corrected.
- Shortages or excess capacities are adjusted.
The MDPS can then be used to train a custom deep learning model.
The practical viability of the environment is evaluated on different test systems featuring mono- to tri-objective problems.
